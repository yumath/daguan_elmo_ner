{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "number of tokens in training data is : 94268535\n"
     ]
    }
   ],
   "source": [
    "corpus = 'data/corpus.txt'\n",
    "ner_train = 'data/train.txt'\n",
    "ner_test = 'data/test.txt'\n",
    "\n",
    "data = []\n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        temp = line.strip().split('_')\n",
    "        data.append(temp)\n",
    "\n",
    "with open(ner_train, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        temp = line.strip().split('  ')\n",
    "        words = []\n",
    "        for token in temp:\n",
    "            word, tag = token.strip().split('/')\n",
    "            words += word.split('_')\n",
    "        data.append(words)\n",
    "            \n",
    "with open(ner_test, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        temp = line.strip().split('_')\n",
    "        data.append(temp)\n",
    "\n",
    "slice_size = int(len(data)/6)+1\n",
    "for i in range(6):\n",
    "    with open('data/train/corpus_%s.txt'%(i+1), 'w', encoding='utf-8') as f:\n",
    "        for sent in data[slice_size*i:slice_size*(i+1)]:\n",
    "            f.write(' '.join(sent))\n",
    "            f.write('\\n')\n",
    "print('done')\n",
    "\n",
    "n_tokens = 0\n",
    "for sent in data:\n",
    "    n_tokens += len(sent)\n",
    "print('number of tokens in training data is : {}'.format(n_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7384', '4846', '2699', '12772', '3948', '3009', '3166', '882', '16688', '2680', '9685', '11487', '8197', '14126', '17166', '20743', '15462', '11033', '12985', '11642', '19453', '9942', '21224', '14747', '14764', '5815', '3166', '14244', '1886', '14123', '14077', '4581', '3253', '14796', '12540', '2680', '18826', '6102', '4779', '7106', '8197', '14752', '6303', '15274']]\n"
     ]
    }
   ],
   "source": [
    "print(data[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<S>', 1000000002), ('</S>', 1000000001), ('<UNK>', 1000000000), ('21224', 4097984), ('8197', 2453362), ('15274', 1778007), ('7664', 1117514), ('7706', 770265), ('4246', 720654), ('19311', 716206)]\n"
     ]
    }
   ],
   "source": [
    "dico = {}\n",
    "for sent in data:\n",
    "    for word in sent:\n",
    "        if word not in dico:\n",
    "            dico[word] = 1\n",
    "        else:\n",
    "            dico[word] += 1\n",
    "dico['<S>'] = 1000000002\n",
    "dico['</S>'] = 1000000001\n",
    "dico['<UNK>'] = 1000000000\n",
    "sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "print(sorted_items[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21151 words in vocab last time\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open('data/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    for i in f:\n",
    "        count+=1\n",
    "print(\"{} words in vocab last time\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 21206 words in vocab now!\n"
     ]
    }
   ],
   "source": [
    "print('we have {} words in vocab now!'.format(len(sorted_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max char per token is 5\n"
     ]
    }
   ],
   "source": [
    "max_char_per_token = 0\n",
    "for item, count in sorted_items:\n",
    "    if len(item) > max_char_per_token:\n",
    "        max_char_per_token = len(item)\n",
    "print('max char per token is {}'.format(max_char_per_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done !\n"
     ]
    }
   ],
   "source": [
    "with open('data/vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for item, count in sorted_items:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "print('done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
